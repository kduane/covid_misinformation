{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve, roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(subreddit, n):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    if n < 100:\n",
    "        params = {\n",
    "        'subreddit' : subreddit, \n",
    "        'size': n \n",
    "        }\n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "    else:\n",
    "# note:  Pushshift.io now has a hard limit of 100 posts returned per API hit, so I'm setting this 100 limit here and will loop through this call until I hit n posts\n",
    "        #get now in epoch date time format\n",
    "        today = datetime.now()\n",
    "        now = today.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        epoch = int(now.timestamp()) #get now in epoch date time format\n",
    "\n",
    "        params = {\n",
    "            'subreddit' : subreddit,\n",
    "            'size' : 100, #pull 100 posts at a time\n",
    "            'before' : epoch #set to now\n",
    "        }\n",
    "        posts = []\n",
    "        # until I have as many posts as called for\n",
    "        while len(posts) <  n:\n",
    "            # get the posts\n",
    "            res = requests.get(url, params)\n",
    "            # convert to list\n",
    "            data = res.json()\n",
    "            # add to list\n",
    "            print(data['data'][99]['created_utc'])\n",
    "            posts.extend(data['data'])\n",
    "            print(len(posts))\n",
    "            # set params 'before' to oldest post's utc\n",
    "            params['before'] = data['data'][99]['created_utc']\n",
    "            # pause for 5 seconds so we're not hitting the API too fast and maxing it out.\n",
    "            time.sleep(5)\n",
    "\n",
    "    return pd.DataFrame(posts) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600971276\n",
      "100\n",
      "1600951109\n",
      "200\n",
      "1600917845\n",
      "300\n",
      "1600894509\n",
      "400\n",
      "1600876783\n",
      "500\n",
      "1600861216\n",
      "600\n",
      "1600825421\n",
      "700\n",
      "1600802468\n",
      "800\n",
      "1600745658\n",
      "900\n",
      "1600716276\n",
      "1000\n",
      "1600692610\n",
      "1100\n",
      "1600649803\n",
      "1200\n",
      "1600625979\n",
      "1300\n",
      "1600588448\n",
      "1400\n",
      "1600551005\n",
      "1500\n",
      "1600523384\n",
      "1600\n",
      "1600486554\n",
      "1700\n",
      "1600458074\n",
      "1800\n",
      "1600438346\n",
      "1900\n",
      "1600382746\n",
      "2000\n",
      "1600361450\n",
      "2100\n",
      "1600346397\n",
      "2200\n",
      "1600319134\n",
      "2300\n",
      "1600300535\n",
      "2400\n",
      "1600280035\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "reddit_coronavirus_posts = get_posts('coronavirus', 2_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_coronavirus_posts['title'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.to_csv('./datasets/reddit_coronavirus_titles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
       "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
       "       'author_premium', 'awarders', 'can_mod_post', 'contest_mode',\n",
       "       'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
       "       'is_crosspostable', 'is_meta', 'is_original_content',\n",
       "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
       "       'link_flair_background_color', 'link_flair_richtext',\n",
       "       'link_flair_template_id', 'link_flair_text', 'link_flair_text_color',\n",
       "       'link_flair_type', 'locked', 'media_only', 'no_follow', 'num_comments',\n",
       "       'num_crossposts', 'over_18', 'parent_whitelist_status', 'permalink',\n",
       "       'pinned', 'post_hint', 'preview', 'pwls', 'retrieved_on', 'score',\n",
       "       'selftext', 'send_replies', 'spoiler', 'stickied', 'subreddit',\n",
       "       'subreddit_id', 'subreddit_subscribers', 'subreddit_type',\n",
       "       'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width',\n",
       "       'title', 'total_awards_received', 'treatment_tags', 'upvote_ratio',\n",
       "       'url', 'url_overridden_by_dest', 'whitelist_status', 'wls',\n",
       "       'link_flair_css_class', 'media', 'media_embed', 'removed_by_category',\n",
       "       'secure_media', 'secure_media_embed', 'author_flair_background_color',\n",
       "       'author_flair_text_color', 'author_flair_template_id', 'distinguished',\n",
       "       'author_cakeday', 'crosspost_parent', 'crosspost_parent_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_coronavirus_posts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_5_broken(cell):\n",
    "    if \"Rule 5:\" in cell: # Rule 5 in the r/coronavirus subreddit is 'keep information quality high'\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                Europe\n",
       "1                               Central &amp; East Asia\n",
       "2                                                Europe\n",
       "3                                         Latin America\n",
       "4       Removed - Rule 5: Keep information quality high\n",
       "                             ...                       \n",
       "2495                                                USA\n",
       "2496                                    Academic Report\n",
       "2497                                                USA\n",
       "2498                                              World\n",
       "2499                                                USA\n",
       "Name: link_flair_text, Length: 2500, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_coronavirus_posts['link_flair_text'] = reddit_coronavirus_posts['link_flair_text'].fillna(\"\")\n",
    "reddit_coronavirus_posts['link_flair_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4                         Meanwhile in The Netherlands...\n",
       "5       Japanese firm develops first UV lamp that safe...\n",
       "43              The Overwhelming Racism Of COVID Coverage\n",
       "55               The Second COVID-19 Wave is Already Here\n",
       "71      Unilever, Consumer Giants Push Suppliers to Re...\n",
       "                              ...                        \n",
       "2472    Opinion: Big Ten's decision to play football s...\n",
       "2476    Racebaiter Hilary Brueck and Business Insider ...\n",
       "2477    Echo from the past: China has been telling us ...\n",
       "2488    CDC Director: Masks Are 'The Most Important, P...\n",
       "2490          Quick News: Trump Calls for Fiscal Stimulus\n",
       "Name: title, Length: 173, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_coronavirus_posts[reddit_coronavirus_posts['link_flair_text'].map(rule_5_broken) == True]['title']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit (conda)",
   "language": "python",
   "name": "python37764bitconda77dc9d652bc147c6a9d0ea8af6b5ba0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
